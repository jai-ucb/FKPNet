{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "import zlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import rc\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "np.random.seed(42)\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    df, data_path = {}, data_path\n",
    "    for f, l, n, t in zip(['IdLookupTable.csv', 'SampleSubmission.csv', 'test.csv', 'training.csv'],\n",
    "                            ['id_lookup', 'sample_submission', 'test', 'train'],\n",
    "                            [['row_id', 'image_id', 'feature_name', 'location'],\n",
    "                                ['row_id', 'location'],\n",
    "                                ['image_id', 'image'],\n",
    "                                ['left_eye_center_x', 'left_eye_center_y', 'right_eye_center_x', 'right_eye_center_y', 'left_eye_inner_corner_x', 'left_eye_inner_corner_y', \n",
    "                                'left_eye_outer_corner_x', 'left_eye_outer_corner_y', 'right_eye_inner_corner_x', 'right_eye_inner_corner_y', 'right_eye_outer_corner_x', \n",
    "                                'right_eye_outer_corner_y', 'left_eyebrow_inner_end_x', 'left_eyebrow_inner_end_y', 'left_eyebrow_outer_end_x', 'left_eyebrow_outer_end_y', \n",
    "                                'right_eyebrow_inner_end_x', 'right_eyebrow_inner_end_y', 'right_eyebrow_outer_end_x', 'right_eyebrow_outer_end_y', 'nose_tip_x', 'nose_tip_y', \n",
    "                                'mouth_left_corner_x', 'mouth_left_corner_y', 'mouth_right_corner_x', 'mouth_right_corner_y', 'mouth_center_top_lip_x', 'mouth_center_top_lip_y', \n",
    "                                'mouth_center_bottom_lip_x', 'mouth_center_bottom_lip_y', 'image']],\n",
    "                                [{'row_id':'uint16', 'image_id':'uint16', 'location':'float32'},\n",
    "                                {'row_id':'uint16', 'location':'float32'},\n",
    "                                {'image_id':'uint16', 'image':'object'},\n",
    "                                {'left_eye_center_x':'float32', 'left_eye_center_y':'float32', 'right_eye_center_x':'float32', 'right_eye_center_y':'float32', \n",
    "                                'left_eye_inner_corner_x':'float32', 'left_eye_inner_corner_y':'float32', 'left_eye_outer_corner_x':'float32', 'left_eye_outer_corner_y':'float32', \n",
    "                                'right_eye_inner_corner_x':'float32', 'right_eye_inner_corner_y':'float32', 'right_eye_outer_corner_x':'float32', 'right_eye_outer_corner_y':'float32', \n",
    "                                'left_eyebrow_inner_end_x':'float32', 'left_eyebrow_inner_end_y':'float32', 'left_eyebrow_outer_end_x':'float32', 'left_eyebrow_outer_end_y':'float32', \n",
    "                                'right_eyebrow_inner_end_x':'float32', 'right_eyebrow_inner_end_y':'float32', 'right_eyebrow_outer_end_x':'float32', 'right_eyebrow_outer_end_y':'float32', \n",
    "                                'nose_tip_x':'float32', 'nose_tip_y':'float32', 'mouth_left_corner_x':'float32', 'mouth_left_corner_y':'float32', 'mouth_right_corner_x':'float32', \n",
    "                                'mouth_right_corner_y':'float32', 'mouth_center_top_lip_x':'float32', 'mouth_center_top_lip_y':'float32', 'mouth_center_bottom_lip_x':'float32', \n",
    "                                'mouth_center_bottom_lip_y':'float32', 'image':'object'}]):\n",
    "        print(\"Loading file '%s'...\" % \"\".join( (data_path, f)))\n",
    "        df[l] = pd.read_csv(\"\".join( (data_path, f) ), names = n, dtype = t, skiprows = 1)\n",
    "        if \"image\" in df[l]:\n",
    "            print(\"\\tProcessing %d images...\" % df[l].shape[0])\n",
    "            df[l]['image'] = df[l][\"image\"].map(lambda x: np.array(list(map(int, x.split(\" \")))))\n",
    "        print(\"\\tDone!  shape:\", df[l].shape, \"\\n\")\n",
    "\n",
    "    print(\"All data loaded in to dataframe 'df'.\")\n",
    "    return df # df['training'] and df['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_extreme(df_img):\n",
    "    idx_vals = df_img.index.values\n",
    "    df_rng = {}\n",
    "    for c in [c for c in df_img.columns if c.endswith('_x') | c.endswith('_y')]:\n",
    "        df_rng[c] = {'min':np.nanmin(df_img[c].values), 'max':np.nanmax(df_img[c].values),\n",
    "            'avg':np.nanmean(df_img[c].values)}\n",
    "    df_rng = pd.DataFrame(df_rng).T\n",
    "\n",
    "    cols = ['min_value', 'max_value', 'min_img', 'max_img', 'min_idx', 'max_idx']\n",
    "    extremes = {}\n",
    "    for i in df_rng.index.values:\n",
    "        min_val, max_val = df_rng.loc[i]['min'], df_rng.loc[i]['max']\n",
    "        min_img = df_img[(df_img[i] == min_val)].image.values[0]\n",
    "        max_img = df_img[(df_img[i] == max_val)].image.values[0]\n",
    "        min_idx = df_img[(df_img[i] == min_val)].index.values\n",
    "        max_idx = df_img[(df_img[i] == max_val)].index.values\n",
    "        extremes[i] = [min_val, max_val, min_img, max_img, min_idx, max_idx]\n",
    "\n",
    "    df_extremes = pd.DataFrame(extremes).T\n",
    "    df_extremes.columns = cols\n",
    "\n",
    "    # add 1st flag for preprocessing : isBadImage\n",
    "    extremeIndex = []\n",
    "    for keypoint in extremes.keys():\n",
    "        minIndex = extremes[keypoint][4]\n",
    "        maxIndex = extremes[keypoint][5]\n",
    "        if minIndex not in extremeIndex:\n",
    "            extremeIndex.append(minIndex[0])\n",
    "        if maxIndex not in extremeIndex:\n",
    "            extremeIndex.append(maxIndex[0])\n",
    "    extremeArray = np.zeros(7049)\n",
    "    for i in extremeIndex:\n",
    "        extremeArray[i] = 1\n",
    "    df_img['is_extreme_image'] = extremeArray\n",
    "    return df_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_missing_pixel(df):\n",
    "    # add 2nd flag for preprocessing : isMissingPixel \n",
    "    isMissingPixel = np.zeros(df.shape[0])\n",
    "    df['is_missing_pixel'] = isMissingPixel\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_missing_keypoints(df):\n",
    "    # add 3rdflag for preprocessing : missing keypoints\n",
    "    cols = [c for c in df.columns if 'image' not in c]\n",
    "    missing = pd.DataFrame(df[cols].isnull().sum().sort_values(ascending = False)).reset_index()\n",
    "    missing.columns = ['keypoint', 'num_missing']\n",
    "    missing['pct_missing'] = (missing.num_missing / df.shape[0]).astype(np.float32)\n",
    "    missing['pct_present'] = 1 - missing.pct_missing\n",
    "\n",
    "    cols = [c for c in df.columns if c.endswith('_x') | c.endswith('_y')]\n",
    "    df_temp = df.copy()[cols]\n",
    "    df_temp = df_temp.isnull().sum(axis=1).reset_index()\n",
    "    df['num_missing_keypoints'] = df_temp[0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_duplicate(df):\n",
    "    # add 4th flag for preprocesssing : has_duplicate (averaged keypoint values)\n",
    "    # Evaluate the presence of duplicate images\n",
    "\n",
    "    df['hash_image'] = df.image.map(lambda x: zlib.adler32(x))\n",
    "    df_dupes_hash = pd.DataFrame(df.groupby(by='hash_image').index.count().sort_values()).reset_index()\n",
    "    df_dupes_hash.columns = ['hash_image', 'frequency']\n",
    "    df_dupes_hash = df_dupes_hash[(df_dupes_hash.frequency > 1)]\n",
    "    df_dupes_hash = pd.merge(df_dupes_hash, df[['index', 'hash_image']],  how = 'left', on=['hash_image']).sort_values(by=['frequency', 'hash_image'], ascending = False)\n",
    "    df.drop(columns=['hash_image'], inplace=True)\n",
    "    print(\"TRAIN has %d duplicate rows (%d unique images)\" % (len(df_dupes_hash), len(np.unique(df_dupes_hash.hash_image))))\n",
    "    \n",
    "    df_train_dupes = pd.DataFrame(df_dupes_hash.groupby(by='frequency').index.count()).sort_values(by='index', ascending = False)\n",
    "    df_train_dupes.columns = ['duplicates']\n",
    "    df_train_dupes['cuml_pct'] = round(df_train_dupes.cumsum()/df_train_dupes.duplicates.sum() * 100,2)\n",
    "\n",
    "    # Do the duplicated images at least having matching labels?\n",
    "    first_duped_image_hash = df_dupes_hash.iloc[0, df_dupes_hash.columns.get_loc('hash_image')]\n",
    "    print(df_dupes_hash)\n",
    "    print(first_duped_image_hash)\n",
    "    train_dupes_idx = df_dupes_hash.loc[(df_dupes_hash.hash_image == first_duped_image_hash)]['index'].values\n",
    "    print(train_dupes_idx)\n",
    "    cols = [c for c in df.columns if c.endswith('_x') | c.endswith('_y')]\n",
    "\n",
    "    def highlight_max(s):\n",
    "        is_max = s == s.max()\n",
    "        return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "    df.loc[(df['index'].isin(train_dupes_idx))][cols].style\\\n",
    "        .apply(highlight_max)\\\n",
    "        .set_na_rep(\"N/A\").format(None, na_rep=\"N/A\").highlight_null(berkeley_palette['south_hall'])\n",
    "    \n",
    "    # It looks like the duplicated images in TRAIN don't reliably have the same truth label values (above)\n",
    "    # One option would be for us to merge these images together, and average the coordinates acorss all labels.\n",
    "    # Here's what that looks like:\n",
    "\n",
    "    pd.DataFrame(df.loc[(df['index'].isin(train_dupes_idx))][cols].dropna(axis='columns').mean()).T.style\\\n",
    "        .set_properties(**{'background-color':'black',\n",
    "                            'color': 'lawngreen',\n",
    "                            'border-color':'white'})\n",
    "    def check_dup(img, ref):\n",
    "        if img not in ref:\n",
    "            ref.append(img)\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    # de-dupe copied of TRAIN and TEST \n",
    "    ref = []\n",
    "    df = df.reset_index()\n",
    "    df['hash_image'] = df.image.map(lambda x: zlib.adler32(x))\n",
    "    df['is_duplicate'] = df.hash_image.map(lambda x: check_dup(x, ref))\n",
    "    df = df.drop(['index', 'hash_image'], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
